{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171e28f8",
   "metadata": {},
   "source": [
    "# <font face = 'Impact' color = '#FFAEBC' > Sample Demonstration on Machine Learning for Regression<font/>\n",
    "#### <font face = 'Times New Roman' color = '#B5E5CF'> License: GPL v3.0<font/>\n",
    "#### <font face = 'Times New Roman' color = '#B5E5CF'> Author and Trainer: Paolo Hilado MSc. (Data Science)<font/>\n",
    "This notebook provides a backgrounder in doing Machine Learning in Python employing models such as Ridge Regression, LASSO Regression, Elastic Net, and Random Forest Regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f4ee2",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Business Understanding:<font/>\n",
    "Management seeks to gain a deeper understanding of the factors that drive employee productivity across the organization. The Human Resources and Development department has provided a comprehensive employee dataset containing demographic, performance, and engagement-related variables. The primary business objective is to DEVELOP A PREDICTIVE MODEL that accurately estimates an employee’s ProductivityScore based on key predictors such as age, department, tenure, education level, remote work ratio, job satisfaction, work hours, project load, managerial feedback, training participation, promotion history, and recent performance ratings.\n",
    "\n",
    "By identifying and quantifying the most influential drivers of productivity, the organization aims to:\n",
    "- Improve workforce management and development strategies,\n",
    "- Optimize training and performance review programs,\n",
    "- Support data-driven decision-making in promotions, hiring, and resource allocation, and\n",
    "- Enhance overall organizational efficiency and employee satisfaction.\n",
    "\n",
    "The success of this initiative will be measured by the model’s ability to accurately predict productivity and provide actionable insights that inform HR and management policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # used for training and testing a model\n",
    "import math # used to separate the whole number from the decimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be94b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "df = pd.read_csv(\"employee_productivity_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cf3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.eq(' ').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5532821",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Data Understanding:<font/>\n",
    "   \n",
    "The dataset provided by the Human Resources and Development department contains detailed records for all employees within the organization. The primary purpose of this data is to support an analytical exploration of factors influencing employee productivity, represented by the response variable ProductivityScore.\n",
    "\n",
    "The dataset includes a mix of demographic, behavioral, and performance-related variables that may influence productivity. The predictor variables are as follows:\n",
    "- Age – The employee’s age in years, representing workforce demographics.\n",
    "- Department – The functional area or division where the employee works (categorical).\n",
    "- YearsAtCompany – The number of years the employee has been with the organization, reflecting experience and organizational familiarity.\n",
    "- EducationLevel – The highest education qualification level attained (ordinal).\n",
    "- RemoteWorkRatio – The proportion of work done remotely, indicating work flexibility.\n",
    "- JobSatisfactionScore – A self-reported or survey-based score indicating job satisfaction.\n",
    "- AverageWeeklyHours – The average number of hours worked per week.\n",
    "- NumProjects – The number of projects currently or recently handled by the employee.\n",
    "- ManagerFeedbackScore – The manager’s performance feedback rating.\n",
    "- TrainingHoursLastYear – The total number of hours spent in training over the past year.\n",
    "- PromotionsLast5Years – The number of promotions received within the last five years.\n",
    "- RecentPerformanceRating – The most recent formal performance appraisal score.\n",
    "\n",
    "The dataset is expected to include both numerical and categorical data types, potentially with varying scales and distributions. Before modeling, the data will need to be explored and preprocessed to ensure quality and reliability. This will involve checking for missing values, outliers, inconsistent data entries, and correlations among variables. Exploratory Data Analysis (EDA) will also be performed to uncover patterns, relationships, and possible drivers of productivity.\n",
    "\n",
    "Understanding these characteristics will guide appropriate feature engineering, data transformation, and model selection steps to ensure that the resulting machine learning model accurately reflects the underlying dynamics of employee productivity within the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c664ea-d11a-401a-acec-6aef237dcf0e",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Data Preparation:<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11164c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the irrelevant feature for developing the machine learning model.\n",
    "df = df.drop(['EmployeeID'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets.\n",
    "# Given 12 explanatory variables we would at need > 146 observations for\n",
    "# training a regression model (Tabachnick and Fidell, 2013). The 70-30 split\n",
    "# will be used for this project. \n",
    "train, test = train_test_split(df, test_size=0.30, random_state=42)\n",
    "print(f'''The number of records for the train set is {len(train)}.\n",
    "The number of records for the test set is {len(test)}.''')\n",
    "# Source: Tabachnick, B.G.,Fidell, L.S., 2013. Using Multivariate Statistics, \n",
    "#         6th ed. Pearson Education, Inc., Boston. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2193cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the explanatory variables from the outcome variable.\n",
    "x_train = train.drop(['ProductivityScore'], axis = 1)\n",
    "y_train = train['ProductivityScore']\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d700a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the explanatory variables from the outcome variable.\n",
    "x_test = test.drop(['ProductivityScore'], axis = 1)\n",
    "y_test = test['ProductivityScore']\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20133f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all the continuous variables.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assigning feature labels to variable continuous_vars.\n",
    "continuous_vars = ['Age','YearsAtCompany', 'RemoteWorkRatio', 'JobSatisfactionScore',\n",
    "                   'AverageWeeklyHours','NumProjects', 'ManagerFeedbackScore', 'TrainingHoursLastYear',\n",
    "                  'PromotionsLast5Years', 'RecentPerformanceRating']\n",
    "\n",
    "# Initialize StandardScaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the continuous variables and transform them.\n",
    "x_train[continuous_vars] = scaler.fit_transform(x_train[continuous_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e40520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all the continuous variables.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have your data in a DataFrame called df with continuous variables\n",
    "# Replace continuous_vars with the names of your continuous variables\n",
    "continuous_vars = ['Age','YearsAtCompany', 'RemoteWorkRatio', 'JobSatisfactionScore',\n",
    "                   'AverageWeeklyHours','NumProjects', 'ManagerFeedbackScore', 'TrainingHoursLastYear',\n",
    "                  'PromotionsLast5Years', 'RecentPerformanceRating']\n",
    "\n",
    "# Fit scaler to the continuous variables and transform them\n",
    "x_test[continuous_vars] = scaler.transform(x_test[continuous_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62dc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Checking for Multicollinearity among continuous variables using correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(x_train[continuous_vars].corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe0195-dbdc-49ed-8e89-5d01ae473ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables automatically\n",
    "x_train = pd.get_dummies(x_train, drop_first=True)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06351bd-d6c4-48c1-aa16-dc384f4c01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables automatically\n",
    "x_test = pd.get_dummies(x_test, drop_first=True)\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6d5c1-b8fb-4183-b86e-9e6a5f5141ef",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Modelling: Ridge Regression<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a machine learning model for a regression problem using the x_train dataset and the\n",
    "# outcome variable y_train.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge # You can replace Ridge with any other regression model you want to tune\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming you have your features in X and target variable in y\n",
    "\n",
    "# Define Ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0],  # Regularization strength (L2 penalty)\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']  # Solver options\n",
    "}\n",
    "# Perform cross-validation grid search\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error' ) # cv=5 for 5-fold cross-validation\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "# CV RMSE of best model\n",
    "cv_rmse = -grid_search.best_score_  # negate because sklearn uses \"maximize\" convention\n",
    "print(\"Mean 5-fold CV RMSE:\", np.round(cv_rmse,2))\n",
    "\n",
    "# Evaluate the best model on the train set using RMSE\n",
    "y_train_pred = best_model.predict(x_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))  # RMSE on train set\n",
    "print(\"Root Mean Squared Error on train set:\", np.round(rmse_train,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dface52-f81c-4413-a061-af9bf8dfc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set using RMSE\n",
    "y_test_pred = best_model.predict(x_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))  # RMSE on test set\n",
    "print(\"Root Mean Squared Error on test set:\", np.round(rmse_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f1acb-ebfe-4ee2-bc33-9e9dc8fedc80",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Modelling: LASSO Regression<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1de1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assuming you have your features in X and target variable in y\n",
    "\n",
    "# Define the Lasso regression model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform cross-validation grid search\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error' )\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "# CV RMSE of best model\n",
    "cv_rmse = -grid_search.best_score_  # negate because sklearn uses \"maximize\" convention\n",
    "print(\"Mean 5-fold CV RMSE:\", np.round(cv_rmse,2))\n",
    "\n",
    "# Evaluate the best model on the train set using RMSE\n",
    "y_train_pred = best_model.predict(x_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))  # RMSE on train set\n",
    "print(\"Root Mean Squared Error on train set:\", np.round(rmse_train,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa0dac-7bcd-4586-bcc9-61e499d0e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set using RMSE\n",
    "y_test_pred = best_model.predict(x_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))  # RMSE on test set\n",
    "print(\"Root Mean Squared Error on test set:\", np.round(rmse_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faab5e-0ec3-43ff-9563-c069c583a327",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Modelling: Elastic Net<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Elastic Net Regression\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# (You should replace this with your own dataset)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Elastic Net\n",
    "parametersGrid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"l1_ratio\": np.arange(0.1, 0.9, 0.1)\n",
    "}\n",
    "\n",
    "# Initialize the Elastic Net model\n",
    "eNet = ElasticNet()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search  = GridSearchCV(eNet, parametersGrid, scoring='neg_root_mean_squared_error', cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "# CV RMSE of best model\n",
    "cv_rmse = -grid_search.best_score_  # negate because sklearn uses \"maximize\" convention\n",
    "print(\"Mean 5-fold CV RMSE:\", np.round(cv_rmse,2))\n",
    "\n",
    "# Evaluate the best model on the train set using RMSE\n",
    "y_train_pred = best_model.predict(x_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))  # RMSE on train set\n",
    "print(\"Root Mean Squared Error on train set:\", np.round(rmse_train,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191685bd-aa70-4c1f-b96d-f531d9d13d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set using RMSE\n",
    "y_test_pred = best_model.predict(x_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))  # RMSE on test set\n",
    "print(\"Root Mean Squared Error on test set:\", np.round(rmse_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5a2e2-3139-4446-9382-0012aa6e78b4",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Modelling: Random Forest Regressor<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define Random Forest regressor\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error' )\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# CV RMSE of best model\n",
    "cv_rmse = -grid_search.best_score_  # negate because sklearn uses \"maximize\" convention\n",
    "print(\"Mean 5-fold CV RMSE:\", np.round(cv_rmse,2))\n",
    "# Evaluate the best model on the train set using RMSE\n",
    "y_train_pred = best_model.predict(x_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))  # RMSE on train set\n",
    "print(\"Root Mean Squared Error on train set:\", np.round(rmse_train,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944db95-2ab9-4d0b-9289-1725562a9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set using RMSE\n",
    "y_test_pred = best_model.predict(x_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))  # RMSE on test set\n",
    "print(\"Root Mean Squared Error on test set:\", np.round(rmse_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1218612-7635-4bf1-b0bc-686487da3e82",
   "metadata": {},
   "source": [
    "# <font face = 'Palatino Linotype' color = '#5885AF'> Saving the Model for Future Deployment<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfbbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the Random Forest Model.\n",
    "import pickle\n",
    "pickle.dump(best_model, open('RFEMPmodel.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
